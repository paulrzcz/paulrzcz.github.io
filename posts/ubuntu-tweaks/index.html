<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>OpenZFS and Ubuntu tweaks | PaulRz</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="Recently, I embarked on a project to optimize my home server setup by transitioning my disk storage pool from RAID5 using mdadm and XFS to ZFS RAIDZ-1. Honestly, it‚Äôs been a huge improvement in both performance and functionality, and I‚Äôm pretty excited about the results so far. My home server functions as a lab environment, where I primarily serve videos, experiment with Kubernetes clusters, and run various virtual machines (VMs) ‚Äî mostly OpenBSD and FreeBSD instances.">
<meta name="generator" content="Hugo 0.119.0-DEV">


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">



<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />

 
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KJ2QL1MQGP"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-KJ2QL1MQGP', { 'anonymize_ip': false });
}
</script>








  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">‚Üê</span>Home</a>
	
	<a href="/posts">Archive</a>
	<a href="/tags">Tags</a>
	<a href="/about">About</a>

	

	
	  <a class="button" href="https://paulrz.cz/index.xml">Subscribe</a>
	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">OpenZFS and Ubuntu tweaks</h1>

    <div class="tip">
        <time datetime="2024-07-09 00:00:00 &#43;0000 UTC">Jul 9, 2024</time>
        <span class="split">
          ¬∑
        </span>
        <span>
          950 words
        </span>
        <span class="split">
          ¬∑
        </span>
        <span>
          5 minute read
        </span>
    </div>

    
    
        
  
    <aside class="toc">
      <details>
          <summary>Table of Contents
          </summary>
          <div>
              <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#why-the-move-to-zfs-raidz">Why the move to ZFS RAIDZ?</a></li>
        <li><a href="#the-zfs-caching-problem-on-ubuntu">The ZFS caching problem on Ubuntu</a></li>
        <li><a href="#step-1-lower-swappiness">Step 1: Lower Swappiness</a></li>
        <li><a href="#step-2-more-aggressive-file-cache-eviction">Step 2: More Aggressive File Cache Eviction</a></li>
        <li><a href="#step-3-limiting-zfs-arc-cache-size">Step 3: Limiting ZFS ARC Cache Size</a></li>
        <li><a href="#step-4-adding-ssd-based-l2arc-cache">Step 4: Adding SSD-based L2ARC Cache</a></li>
        <li><a href="#final-thoughts">Final Thoughts</a></li>
      </ul>
    </li>
  </ul>
</nav>
          </div>
      </details>
    </aside>
  


    


    <div class="content">
      <p>Recently, I embarked on a project to optimize my home server setup by transitioning my disk storage pool from RAID5 using <code>mdadm</code> and XFS to ZFS RAIDZ-1. Honestly, it‚Äôs been a huge improvement in both performance and functionality, and I‚Äôm pretty excited about the results so far. My home server functions as a lab environment, where I primarily serve videos, experiment with Kubernetes clusters, and run various virtual machines (VMs) ‚Äî mostly OpenBSD and FreeBSD instances.</p>
<h3 id="why-the-move-to-zfs-raidz">Why the move to ZFS RAIDZ? <a href="#why-the-move-to-zfs-raidz" class="anchor">üîó</a></h3><p>I‚Äôve always admired ZFS for its robustness and advanced features. By shifting to ZFS, I gained access to native snapshots, data integrity checks, and better handling of bit rot‚Äîfeatures I was missing in my previous RAID5 setup. ZFS RAIDZ-1 offers a good balance of redundancy and storage efficiency, making it perfect for my needs in a home lab where data integrity matters but full enterprise-level RAID setups (like RAIDZ-2) might be overkill.</p>
<p>Additionally, with ZFS‚Äôs built-in compression and deduplication features, I could more efficiently store data, especially for things like repetitive Docker or VM images.</p>
<h3 id="the-zfs-caching-problem-on-ubuntu">The ZFS caching problem on Ubuntu <a href="#the-zfs-caching-problem-on-ubuntu" class="anchor">üîó</a></h3><p>However, one issue quickly surfaced. ZFS, in its greatness, comes with its own cache management system: the ARC (Adaptive Replacement Cache). On the other hand, Ubuntu‚Äôs regular Linux filesystem has its own caching mechanisms. The two, unfortunately, don‚Äôt talk to each other, which leads to double memory usage and redundant caching. ZFS is already extremely efficient with its ARC, but Linux was layering its own file cache on top of that, causing excessive memory consumption and unnecessary caching.</p>
<p>To solve this, I needed to tweak a few settings to make memory usage more efficient. I wanted to:</p>
<ol>
<li><strong>Limit the ZFS ARC cache</strong> so it wouldn‚Äôt gobble up too much RAM.</li>
<li><strong>Make Linux more aggressive in evicting file system cache</strong> to prevent memory overload.</li>
<li><strong>Add SSD-based L2ARC</strong> for faster reads, using an NVMe SSD, to improve overall system responsiveness.</li>
</ol>
<p>Here‚Äôs how I did it:</p>
<hr>
<h3 id="step-1-lower-swappiness">Step 1: Lower Swappiness <a href="#step-1-lower-swappiness" class="anchor">üîó</a></h3><p>Swappiness determines how aggressively Ubuntu uses swap space. In a home server where I want maximum performance, I don‚Äôt want the system to start swapping prematurely, especially since my server has a decent amount of RAM (32GB). I want Linux to prioritize keeping data in RAM.</p>
<p>I set the swappiness value low by adding this line to <code>/etc/sysctl.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>vm.swappiness<span style="color:#666">=</span><span style="color:#666">10</span>
</span></span></code></pre></div><p>A lower swappiness ensures that RAM is used to its fullest extent before swap is touched. In this case, a value of 10 tells the system to only use swap when absolutely necessary, freeing more RAM for ZFS ARC and other applications.</p>
<hr>
<h3 id="step-2-more-aggressive-file-cache-eviction">Step 2: More Aggressive File Cache Eviction <a href="#step-2-more-aggressive-file-cache-eviction" class="anchor">üîó</a></h3><p>Next, I needed the Linux file system to be more aggressive about releasing cached files when memory is tight. I increased the <code>vfs_cache_pressure</code> value to 200, which tells the system to favor releasing unused file system caches faster.</p>
<p>To do this, I added this line to <code>/etc/sysctl.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>vm.vfs_cache_pressure<span style="color:#666">=</span><span style="color:#666">200</span>
</span></span></code></pre></div><p>By default, this value is set to 100, but bumping it up to 200 forces Linux to evict more cached data when it‚Äôs no longer needed, allowing more room for ZFS ARC.</p>
<hr>
<h3 id="step-3-limiting-zfs-arc-cache-size">Step 3: Limiting ZFS ARC Cache Size <a href="#step-3-limiting-zfs-arc-cache-size" class="anchor">üîó</a></h3><p>One of ZFS‚Äôs powerful features is the ARC (Adaptive Replacement Cache), which dynamically adjusts based on memory usage. However, if left unchecked, ZFS ARC will happily eat up a large chunk of RAM, potentially leading to memory pressure for other services like my Kubernetes cluster or VMs.</p>
<p>To limit the ZFS ARC, I edited the <code>zfs.conf</code> file, which is responsible for setting ZFS kernel module parameters:</p>
<ol>
<li>
<p>Open <code>/etc/modprobe.d/zfs.conf</code> and add the following line to limit the ARC to 4GB:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>options zfs <span style="color:#b8860b">zfs_arc_max</span><span style="color:#666">=</span><span style="color:#666">4294967296</span>
</span></span></code></pre></div><p>This configuration caps the ARC size at 4GB, which I found to be a sweet spot in my setup after some experimentation. ZFS still gets enough RAM to cache frequently accessed data, but it doesn&rsquo;t starve other processes.</p>
</li>
<li>
<p>After saving the file, I ran the following command to update the initial RAM filesystem (initramfs), which ensures the changes take effect on reboot:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo update-initramfs -u
</span></span></code></pre></div></li>
</ol>
<hr>
<h3 id="step-4-adding-ssd-based-l2arc-cache">Step 4: Adding SSD-based L2ARC Cache <a href="#step-4-adding-ssd-based-l2arc-cache" class="anchor">üîó</a></h3><p>For an extra performance boost, especially for read-heavy tasks like serving media files, I added a second layer of cache using an SSD as ZFS‚Äôs L2ARC (Level 2 Adaptive Replacement Cache). This is where ZFS excels: it can use fast SSDs to hold cached data, improving read performance by keeping frequently accessed data closer to the CPU.</p>
<p>In my case, I used a 500GB NVMe SSD for the L2ARC, and the performance difference was immediately noticeable.</p>
<p>To add the SSD as a cache device to the pool, I used the following command:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>zpool add mypool cache /dev/nvme0n1
</span></span></code></pre></div><p>This added the NVMe SSD as an L2ARC to my ZFS pool, where it acts as a secondary cache, speeding up reads significantly by offloading data from the slower spinning disks to the SSD. The combination of ARC and L2ARC creates a hybrid caching system that leverages both RAM and SSD speed.</p>
<hr>
<h3 id="final-thoughts">Final Thoughts <a href="#final-thoughts" class="anchor">üîó</a></h3><p>Moving from <code>mdadm</code> RAID5 with XFS to ZFS RAIDZ-1 has been a game-changer for my home server. Not only am I now benefiting from ZFS‚Äôs advanced features like snapshots and data integrity, but with the added SSD-based L2ARC and careful memory management, my system performs much better under load. Whether it‚Äôs spinning up Kubernetes pods or serving large video files, everything runs smoother and faster now.</p>
<p>ZFS‚Äôs flexibility really shines when you tweak it according to your specific workload, and the combination of ARC, L2ARC, and fine-tuning memory management has turned my home lab into a much more responsive and efficient environment. If you‚Äôre looking to level up your storage game at home, I highly recommend making the switch to ZFS.</p>

    </div>

    
    
    

</section>


    </main>
    
    <footer id="footer">
    
        <div id="social">


    <a class="symbol" href="https://github.com/paulrzcz" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28"  viewBox="0 0 72 72" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    
    <title>Github</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)">
            <g id="Github" transform="translate(264.000000, 939.000000)">
                <path d="M8,72 L64,72 C68.418278,72 72,68.418278 72,64 L72,8 C72,3.581722 68.418278,-8.11624501e-16 64,0 L8,0 C3.581722,8.11624501e-16 -5.41083001e-16,3.581722 0,8 L0,64 C5.41083001e-16,68.418278 3.581722,72 8,72 Z" id="Rounded" fill="#bbbbbb"></path>
                <path d="M35.9985,13 C22.746,13 12,23.7870921 12,37.096644 C12,47.7406712 18.876,56.7718301 28.4145,59.9584121 C29.6145,60.1797862 30.0525,59.4358488 30.0525,58.7973276 C30.0525,58.2250681 30.0315,56.7100863 30.0195,54.6996482 C23.343,56.1558981 21.9345,51.4693938 21.9345,51.4693938 C20.844,48.6864054 19.2705,47.9454799 19.2705,47.9454799 C17.091,46.4500754 19.4355,46.4801943 19.4355,46.4801943 C21.843,46.6503662 23.1105,48.9634994 23.1105,48.9634994 C25.2525,52.6455377 28.728,51.5823398 30.096,50.9649018 C30.3135,49.4077535 30.9345,48.3460615 31.62,47.7436831 C26.2905,47.1352808 20.688,45.0691228 20.688,35.8361671 C20.688,33.2052792 21.6225,31.0547881 23.1585,29.3696344 C22.911,28.7597262 22.0875,26.3110578 23.3925,22.9934585 C23.3925,22.9934585 25.4085,22.3459017 29.9925,25.4632101 C31.908,24.9285993 33.96,24.6620468 36.0015,24.6515052 C38.04,24.6620468 40.0935,24.9285993 42.0105,25.4632101 C46.5915,22.3459017 48.603,22.9934585 48.603,22.9934585 C49.9125,26.3110578 49.089,28.7597262 48.8415,29.3696344 C50.3805,31.0547881 51.309,33.2052792 51.309,35.8361671 C51.309,45.0917119 45.6975,47.1292571 40.3515,47.7256117 C41.2125,48.4695491 41.9805,49.9393525 41.9805,52.1877301 C41.9805,55.4089489 41.9505,58.0067059 41.9505,58.7973276 C41.9505,59.4418726 42.3825,60.1918338 43.6005,59.9554002 C53.13,56.7627944 60,47.7376593 60,37.096644 C60,23.7870921 49.254,13 35.9985,13" fill="#FFFFFF"></path>
            </g>
        </g>
    </g>
</svg>
    </a>

    <a class="symbol" href="https://x.com/paulrzcz" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28" version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="438.536px" height="438.536px" viewBox="0 0 438.536 438.536" style="enable-background:new 0 0 438.536 438.536;"
	 xml:space="preserve">
<g>
	<path d="M414.41,24.123C398.333,8.042,378.963,0,356.315,0H82.228C59.58,0,40.21,8.042,24.126,24.123
		C8.045,40.207,0.003,59.576,0.003,82.225v274.084c0,22.647,8.042,42.018,24.123,58.102c16.084,16.084,35.454,24.126,58.102,24.126
		h274.084c22.648,0,42.018-8.042,58.095-24.126c16.084-16.084,24.126-35.454,24.126-58.102V82.225
		C438.532,59.576,430.49,40.204,414.41,24.123z M335.471,168.735c0.191,1.713,0.288,4.278,0.288,7.71
		c0,15.989-2.334,32.025-6.995,48.104c-4.661,16.087-11.8,31.504-21.416,46.254c-9.606,14.749-21.074,27.791-34.396,39.115
		c-13.325,11.32-29.311,20.365-47.968,27.117c-18.648,6.762-38.637,10.143-59.953,10.143c-33.116,0-63.76-8.952-91.931-26.836
		c4.568,0.568,9.329,0.855,14.275,0.855c27.6,0,52.439-8.565,74.519-25.7c-12.941-0.185-24.506-4.179-34.688-11.991
		c-10.185-7.803-17.273-17.699-21.271-29.691c4.947,0.76,8.658,1.137,11.132,1.137c4.187,0,9.042-0.76,14.56-2.279
		c-13.894-2.669-25.598-9.562-35.115-20.697c-9.519-11.136-14.277-23.84-14.277-38.114v-0.571
		c10.085,4.755,19.602,7.229,28.549,7.422c-17.321-11.613-25.981-28.265-25.981-49.963c0-10.66,2.758-20.747,8.278-30.264
		c15.035,18.464,33.311,33.213,54.816,44.252c21.507,11.038,44.54,17.227,69.092,18.558c-0.95-3.616-1.427-8.186-1.427-13.704
		c0-16.562,5.853-30.692,17.56-42.399c11.703-11.706,25.837-17.561,42.394-17.561c17.515,0,32.079,6.283,43.688,18.846
		c13.134-2.474,25.892-7.33,38.26-14.56c-4.757,14.652-13.613,25.788-26.55,33.402c12.368-1.716,23.88-4.95,34.537-9.708
		C357.458,149.793,347.462,160.166,335.471,168.735z"/>
</g>
</svg>

    </a>


</div>

    

    <div class="copyright">
    
       ¬© Copyright 
       2024 
       <span class="split">
        <svg fill="#bbbbbb" width="15" height="15" version="1.1" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15px" height="15px" viewBox="0 0 15 15">
  <path d="M13.91,6.75c-1.17,2.25-4.3,5.31-6.07,6.94c-0.1903,0.1718-0.4797,0.1718-0.67,0C5.39,12.06,2.26,9,1.09,6.75&#xA;&#x9;C-1.48,1.8,5-1.5,7.5,3.45C10-1.5,16.48,1.8,13.91,6.75z"/>
</svg>
       </span>
       Pavel Ry≈æov
    
    </div>

    
      <div class="powerby">
        Powered by <a href='http://www.gohugo.io/'>Hugo</a> Theme By <a href='https://github.com/nodejh/hugo-theme-mini'>nodejh</a>
      </div>
    
</footer>



  </body>
</html>
